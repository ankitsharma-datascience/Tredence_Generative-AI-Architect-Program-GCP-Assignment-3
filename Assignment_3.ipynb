{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from pinecone import Pinecone as PineconeClient, ServerlessSpec\n",
    "import langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load dataset\n",
    "with open(\"self_critique_loop_dataset.json\", \"r\") as f:\n",
    "    kb_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embedding and pinecone vector databse creation\n",
    "pc = PineconeClient(api_key=pinecone_api_key)\n",
    "\n",
    "index_name = \"assignment3-kb3072\"\n",
    "if index_name not in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(model_name=\"gemini-embedding-001\")\n",
    "#insert into vector db\n",
    "vectors = []\n",
    "for entry in kb_data:\n",
    "    vec = embedding_model.embed_query(entry[\"answer_snippet\"])\n",
    "    vectors.append({\n",
    "        \"id\": entry[\"doc_id\"],\n",
    "        \"values\": vec,\n",
    "        \"metadata\": {\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"snippet\": entry[\"answer_snippet\"],\n",
    "            \"source\": entry[\"source\"]\n",
    "        }\n",
    "    })\n",
    "index.upsert(vectors)\n",
    "\n",
    "#creating langGraph workflow\n",
    "model = init_chat_model(\n",
    "    \"gemini-2.0-flash\",\n",
    "    model_provider=\"google_genai\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "\n",
    "def retrieve_vector_db(question: str):\n",
    "    vec = embedding_model.embed_query(question)\n",
    "    res = index.query(vector=vec, top_k=5, include_metadata=True)\n",
    "    snippets = [f\"[{m['id']}] {m['metadata']['snippet']}\" for m in res[\"matches\"]]\n",
    "    return snippets\n",
    "\n",
    "\n",
    "def chat_completion(question: str, snippets: list):\n",
    "    context = \"\\n\".join(snippets)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Question: {q}\\nContext:\\n{ctx}\\nAnswer with citations.\"\n",
    "    )\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"q\": question, \"ctx\": context})\n",
    "\n",
    "# Adding Self-Critique Node\n",
    "def critique_answer(answer: str, snippets: list):\n",
    "    critique_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given answer:\\n{ans}\\n\\nCheck if COMPLETE based on snippets:\\n{ctx}\\n\\nReply with either 'COMPLETE' or 'REFINE: <missing keywords>'\"\n",
    "    )\n",
    "    chain = critique_prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"ans\": answer, \"ctx\": '\\n'.join(snippets)})\n",
    "\n",
    "# Adding Refinement Node\n",
    "def refine_answer(question: str, answer: str, critique: str):\n",
    "    if \"COMPLETE\" in critique:\n",
    "        return answer\n",
    "    else:\n",
    "        keyword = critique.replace(\"REFINE:\", \"\").strip()\n",
    "        vec = embedding_model.embed_query(keyword)\n",
    "        res = index.query(vector=vec, top_k=1, include_metadata=True)\n",
    "        extra_snippet = res[\"matches\"][0][\"metadata\"][\"snippet\"]\n",
    "        refined = chat_completion(question, [answer, extra_snippet])\n",
    "        return refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are best practices for caching?\",\n",
    "    \"How should I set up CI/CD pipelines?\",\n",
    "    \"What are performance tuning tips?\",\n",
    "    \"How do I version my APIs?\",\n",
    "    \"What should I consider for error handling?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Query: {q}\")\n",
    "    snippets = retrieve_vector_db(q)\n",
    "    ans = chat_completion(q, snippets)\n",
    "    critique = critique_answer(ans, snippets)\n",
    "    final_ans = refine_answer(q, ans, critique)\n",
    "    print(\"Final Answer:\\n\", final_ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
